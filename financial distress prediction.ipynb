{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "# Reading the dataset\n",
    "data = pd.read_csv(\"../input/Financial Distress.csv\")\n",
    "\n",
    "# Checking the dataset\n",
    "data.head()\n",
    "data.tail()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of companies:\",data.Company.unique().shape)\n",
    "data = data[data.columns.drop(list(data.filter(regex='x80')))] # Since it is a categorical feature with 37 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating target vector and feature matrix\n",
    "Y = data.iloc[:,2].values\n",
    "for y in range(0,len(Y)): # Coverting target variable from continuous to binary form\n",
    "       if Y[y] > -0.5:\n",
    "              Y[y] = 0\n",
    "       else:\n",
    "              Y[y] = 1\n",
    "X = data.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of observations for Healthy and and Bankrupt Companies:\n",
    "num_zeros = 0\n",
    "for num in Y:\n",
    "       if num == 0:\n",
    "              num_zeros = num_zeros+1\n",
    "num_ones = len(Y) - num_zeros \n",
    "\n",
    "print(\"Number of observations for BANKRUPT companies(1's):\",num_ones)\n",
    "print(\"Number of observations for HEALTHY companies(0's):\",num_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y, test_size = 0.30, random_state = 0)\n",
    "X_train_wo_sampling = X_train\n",
    "y_train_wo_sampling = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating more samples units for the bankrupt companies(undersampled data)\n",
    "y_train = (np.matrix(y_train)).T\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = [\"Financial_Distress\"]\n",
    "X_train = pd.DataFrame(X_train)\n",
    "frame = [X_train,y_train]\n",
    "train_data = pd.concat(frame,axis = 1)\n",
    "bankrupt_companies = train_data[train_data.Financial_Distress == 1]\n",
    "\n",
    "feat_mat = bankrupt_companies.iloc[:,:-1].values\n",
    "response = bankrupt_companies.iloc[:,-1].values\n",
    "col_mean = np.zeros(shape=(82,1)) \n",
    "col_std = np.zeros(shape=(82,1)) \n",
    "Dim_1 = np.shape(feat_mat)\n",
    "for i in range(0,Dim_1[1]): # Logic to calculate mean and standard deviation for each column\n",
    "       col_mean[i,0] = np.mean(feat_mat[:,i])\n",
    "       col_std[i,0] = np.std(feat_mat[:,i])\n",
    "col_mean_and_std = np.hstack((col_mean,col_std))\n",
    "\n",
    "added_data = np.zeros(shape=(1200,Dim_1[1])) \n",
    "for i in range (0,len(col_mean_and_std)):\n",
    "       mean_ = col_mean_and_std[i,0]\n",
    "       std_ = col_mean_and_std[i,1]\n",
    "       added_data[:,i] = np.random.normal(mean_,std_,1200)\n",
    "added_y = np.ones(shape=(1200,1)) # Creating labels for the added data\n",
    "\n",
    "X_resampled = np.vstack((X_train,added_data)) # Combining the original data + added data\n",
    "y_train = np.array(y_train)\n",
    "y_resampled = np.vstack((y_train,added_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_resampled = sc.fit_transform(X_resampled)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting XGBClassifier to the training data: Model_1\n",
    "from xgboost import XGBClassifier\n",
    "classifier_1 = XGBClassifier()\n",
    "classifier_1.fit(X_resampled,y_resampled)\n",
    "\n",
    "# Fitting SVM to the training data: Model 2\n",
    "from sklearn.svm import SVC\n",
    "classifier_2 = SVC(kernel = 'linear', C = 1, probability = True, random_state = random.seed(123)) # poly, sigmoid\n",
    "classifier_2.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Creating and Fitting Random Forest Classifier to the training data: Model 3\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_3 = RandomForestClassifier(n_estimators = 5, criterion = 'entropy')\n",
    "classifier_3.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Fitting classifier to the training data: Model 4 \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_4 = LogisticRegression(penalty = 'l1', random_state = 0)\n",
    "classifier_4.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Fitting Balanced Bagging Classifier to the training data: Model 5\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_5 = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier_5.fit(X_resampled,y_resampled)\n",
    "\n",
    "# Fitting Decision Tree to the training data: Model 6\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_6 = DecisionTreeClassifier()\n",
    "classifier_6.fit(X_resampled,y_resampled)\n",
    "\n",
    "# Fitting Naive Bayes to the training data: Model 7\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_7 = GaussianNB()\n",
    "classifier_7.fit(X_resampled,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the results\n",
    "y_pred_1 = classifier_1.predict(X_test)\n",
    "y_pred_2 = classifier_2.predict(X_test)\n",
    "y_pred_3 = classifier_3.predict(X_test)\n",
    "y_pred_4 = classifier_4.predict(X_test)\n",
    "y_pred_5 = classifier_5.predict(X_test)\n",
    "y_pred_6 = classifier_6.predict(X_test)\n",
    "y_pred_7 = classifier_7.predict(X_test)\n",
    "\n",
    "# Creating the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_1 = confusion_matrix(y_test,y_pred_1)\n",
    "accuracy_1 = (cm_1[0,0]+cm_1[1,1])/len(y_test)\n",
    "\n",
    "cm_2 = confusion_matrix(y_test,y_pred_2)\n",
    "accuracy_2 = (cm_2[0,0]+cm_2[1,1])/len(y_test)\n",
    "\n",
    "cm_3 = confusion_matrix(y_test,y_pred_3)\n",
    "accuracy_3 = (cm_3[0,0]+cm_3[1,1])/len(y_test)\n",
    "\n",
    "cm_4 = confusion_matrix(y_test,y_pred_4)\n",
    "accuracy_4 = (cm_4[0,0]+cm_4[1,1])/len(y_test)\n",
    "\n",
    "cm_5 = confusion_matrix(y_test,y_pred_5)\n",
    "accuracy_5 = (cm_5[0,0]+cm_5[1,1])/len(y_test)\n",
    "\n",
    "cm_6 = confusion_matrix(y_test,y_pred_6)\n",
    "accuracy_6 = (cm_6[0,0]+cm_6[1,1])/len(y_test)\n",
    "\n",
    "cm_7 = confusion_matrix(y_test,y_pred_7)\n",
    "accuracy_7 = (cm_7[0,0]+cm_7[1,1])/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_1, recall_1, f_score_1, support = precision_recall_fscore_support(y_test, y_pred_1, average = None)\n",
    "print(\"\\nFor Model 1 - XGBoost:\")\n",
    "print(\"Precision:\",precision_1)\n",
    "print(\"Recall:\",recall_1)\n",
    "print(\"F-Score:\",f_score_1)\n",
    "print(\"Accuracy_XGBoost:\",accuracy_1*100,'%')\n",
    "\n",
    "precision_2, recall_2, f_score_2, support = precision_recall_fscore_support(y_test, y_pred_2, average = None)\n",
    "print(\"\\nFor Model 2 - SVC:\")\n",
    "print(\"Precision:\",precision_2)\n",
    "print(\"Recall:\",recall_2)\n",
    "print(\"F-Score:\",f_score_2)\n",
    "print(\"Accuracy_SVC:\",accuracy_2*100,'%') \n",
    "\n",
    "precision_3, recall_3, f_score_3, support = precision_recall_fscore_support(y_test, y_pred_3, average = None)\n",
    "print(\"\\nFor Model 3 - Random Forest:\")\n",
    "print(\"Precision:\",precision_3)\n",
    "print(\"Recall:\",recall_3)\n",
    "print(\"F-Score:\",f_score_3)\n",
    "print(\"Accuracy_RF:\",accuracy_3*100,'%') \n",
    "\n",
    "precision_4, recall_4, f_score_4, support = precision_recall_fscore_support(y_test, y_pred_4, average = None)\n",
    "print(\"\\nFor Model 4 - Logistic:\")\n",
    "print(\"Precision:\",precision_4)\n",
    "print(\"Recall:\",recall_4)\n",
    "print(\"F-Score:\",f_score_4)\n",
    "print(\"Accuracy_Logistic:\",accuracy_4*100,'%') \n",
    "\n",
    "precision_5, recall_5, f_score_5, support = precision_recall_fscore_support(y_test, y_pred_5, average = None)\n",
    "print(\"\\nFor Model 5 - BalancedBaggingClassifier:\")\n",
    "print(\"Precision:\",precision_5)\n",
    "print(\"Recall:\",recall_5)\n",
    "print(\"F-Score:\",f_score_5)\n",
    "print(\"Accuracy_BalancedBagging:\",accuracy_5*100,'%')\n",
    "\n",
    "precision_6, recall_6, f_score_6, support = precision_recall_fscore_support(y_test, y_pred_6, average = None)\n",
    "print(\"\\nFor Model 6 - Decision Tree Classifier:\")\n",
    "print(\"Precision:\",precision_6)\n",
    "print(\"Recall:\",recall_6)\n",
    "print(\"F-Score:\",f_score_6)\n",
    "print(\"Accuracy_DecisionTree:\",accuracy_6*100,'%') \n",
    "\n",
    "precision_7, recall_7, f_score_7, support = precision_recall_fscore_support(y_test, y_pred_7, average = None)\n",
    "print(\"\\nFor Model 7 - Naive Bayes Classifier:\")\n",
    "print(\"Precision:\",precision_7)\n",
    "print(\"Recall:\",recall_7)\n",
    "print(\"F-Score:\",f_score_7)\n",
    "print(\"Accuracy_NaiveBayes:\",accuracy_7*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the Accuracies of various models\n",
    "ACCURACY = np.vstack((accuracy_1,accuracy_2,accuracy_3,accuracy_4,accuracy_5,accuracy_6,accuracy_7))\n",
    "number = np.array([1,2,3,4,5,6,7])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(number, ACCURACY, color = 'r', marker = 'o', linewidths = 3)\n",
    "for j in range(0,len(ACCURACY)):\n",
    "       ax.annotate('%0.3f' % (ACCURACY[j]),(number[j], ACCURACY[j]))\n",
    "\n",
    "plt.xlabel('MODELS')    \n",
    "plt.ylabel('ACCURACY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-immune",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the F-Values of various models\n",
    "F_SCORE = np.vstack((f_score_1[1],f_score_2[1],f_score_3[1],f_score_4[1],f_score_5[1],f_score_6[1],f_score_7[1]))\n",
    "number = np.array([1,2,3,4,5,6,7])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(number, F_SCORE, marker = 'x', linewidths = 3)\n",
    "for i in range(0,len(F_SCORE)):\n",
    "       ax.annotate('%0.3f' % (F_SCORE[i]),(number[i], F_SCORE[i]))\n",
    "\n",
    "plt.xlabel('MODELS')    \n",
    "plt.ylabel('F-SCORE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "i = 0\n",
    "\n",
    "probas_1 = classifier_1.predict_proba(X_test)\n",
    "probas_2 = classifier_2.predict_proba(X_test)\n",
    "probas_3 = classifier_3.predict_proba(X_test)\n",
    "probas_4 = classifier_4.predict_proba(X_test)\n",
    "probas_5 = classifier_5.predict_proba(X_test)\n",
    "probas_6 = classifier_6.predict_proba(X_test)\n",
    "probas_7 = classifier_7.predict_proba(X_test)\n",
    "\n",
    "probas = np.vstack((probas_1,probas_2,probas_3,probas_4,probas_5,probas_6,probas_7))\n",
    "\n",
    "# Compute ROC curve and area the curve\n",
    "pointer = [0,1102,2204,3306,4408,5510,6612,7714] \n",
    "for a in range(0,7):\n",
    "       index_1 = pointer[a]\n",
    "       index_2 = pointer[a+1]\n",
    "       fpr, tpr, thresholds = roc_curve(y_test, probas[index_1:index_2, 1])\n",
    "       tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "       tprs[-1][0] = 0.0\n",
    "       roc_auc = auc(fpr, tpr)\n",
    "       aucs.append(roc_auc)\n",
    "       plt.plot(fpr, tpr, lw=2, alpha=0.8,\n",
    "                label='Model %d (AUC = %0.2f)' % (a+1, roc_auc))\n",
    "       \n",
    "       a += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Luck', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing various oversampling techniques\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Random Oversampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled_ros, y_resampled_ros = ros.fit_sample(X_train, y_train)\n",
    "print(sorted(Counter(y_resampled_ros).items()))\n",
    "\n",
    "# Synthetic Minority Oversampling Technique(SMOTE) \n",
    "X_resampled_smote, y_resampled_smote = SMOTE().fit_sample(X_train, y_train)\n",
    "print(sorted(Counter(y_resampled_smote).items()))\n",
    "\n",
    "# Adaptive Synthetic (ADASYN) sampling method\n",
    "X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_sample(X_train, y_train)\n",
    "print(sorted(Counter(y_resampled_adasyn).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "X_resampled_ros = sc.fit_transform(X_resampled_ros)\n",
    "X_resampled_smote = sc.fit_transform(X_resampled_smote)\n",
    "X_resampled_adasyn = sc.fit_transform(X_resampled_adasyn)\n",
    "X_train_wo_sampling = sc.fit_transform(X_train_wo_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Balanced Bagging Classifier to the training data: Model 5\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_ros = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier_ros.fit(X_resampled_ros,y_resampled_ros)\n",
    "\n",
    "classifier_smote = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier_smote.fit(X_resampled_smote,y_resampled_smote)\n",
    "\n",
    "classifier_adasyn = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier_adasyn.fit(X_resampled_adasyn,y_resampled_adasyn)\n",
    "\n",
    "classifier_wo_sampling = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier_wo_sampling.fit(X_train_wo_sampling,y_train_wo_sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the results\n",
    "y_pred_ros = classifier_ros.predict(X_test)\n",
    "y_pred_smote = classifier_smote.predict(X_test)\n",
    "y_pred_adasyn = classifier_adasyn.predict(X_test)\n",
    "y_pred_wo_sampling = classifier_wo_sampling.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_ros = confusion_matrix(y_test,y_pred_ros)\n",
    "accuracy_ros = (cm_ros[0,0]+cm_ros[1,1])/len(y_test)\n",
    "\n",
    "cm_smote = confusion_matrix(y_test,y_pred_smote)\n",
    "accuracy_smote = (cm_smote[0,0]+cm_smote[1,1])/len(y_test)\n",
    "\n",
    "cm_adasyn = confusion_matrix(y_test,y_pred_adasyn)\n",
    "accuracy_adasyn = (cm_adasyn[0,0]+cm_adasyn[1,1])/len(y_test)\n",
    "\n",
    "cm_wo_sampling = confusion_matrix(y_test,y_pred_wo_sampling)\n",
    "accuracy_wo_sampling = (cm_wo_sampling[0,0]+cm_wo_sampling[1,1])/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_ros, recall_ros, f_score_ros, support = precision_recall_fscore_support(y_test, y_pred_ros, average = None)\n",
    "print(\"\\nFor RandomOversampling:\")\n",
    "print(\"Precision:\",precision_ros)\n",
    "print(\"Recall:\",recall_ros)\n",
    "print(\"F-Score:\",f_score_ros)\n",
    "print(\"Accuracy_RandomOversampling:\",accuracy_ros*100,'%')\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_smote, recall_smote, f_score_smote, support = precision_recall_fscore_support(y_test, y_pred_smote, average = None)\n",
    "print(\"\\nFor SMOTE:\")\n",
    "print(\"Precision:\",precision_smote)\n",
    "print(\"Recall:\",recall_smote)\n",
    "print(\"F-Score:\",f_score_smote)\n",
    "print(\"Accuracy_smote:\",accuracy_smote*100,'%')\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_adasyn, recall_adasyn, f_score_adasyn, support = precision_recall_fscore_support(y_test, y_pred_adasyn, average = None)\n",
    "print(\"\\nFor ADASYN:\")\n",
    "print(\"Precision:\",precision_adasyn)\n",
    "print(\"Recall:\",recall_adasyn)\n",
    "print(\"F-Score:\",f_score_adasyn)\n",
    "print(\"Accuracy_adasyn:\",accuracy_adasyn*100,'%')\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_wo_sampling, recall_wo_sampling, f_score_wo_sampling, support = precision_recall_fscore_support(y_test, y_pred_adasyn, average = None)\n",
    "print(\"\\nWithout Sampling:\")\n",
    "print(\"Precision:\",precision_wo_sampling)\n",
    "print(\"Recall:\",recall_wo_sampling)\n",
    "print(\"F-Score:\",f_score_wo_sampling)\n",
    "print(\"Accuracy_wo_sampling:\",accuracy_wo_sampling*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the F-Values of various models\n",
    "F_SCORE = np.vstack((f_score_ros[1],f_score_smote[1],f_score_adasyn[1],f_score_wo_sampling[1],f_score_5[1],)) #,f_score_2[1]\n",
    "number = np.array([1,2,3,4,5])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(number, F_SCORE, marker = 'x', linewidths = 3)\n",
    "for i in range(0,len(F_SCORE)):\n",
    "       ax.annotate('%0.3f' % (F_SCORE[i]),(number[i], F_SCORE[i]))\n",
    "\n",
    "plt.xlabel('Sampling Techniques')    \n",
    "plt.ylabel('F-SCORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-scheduling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
